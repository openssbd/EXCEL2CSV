{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel2CSV converter to import data for SSBD-OMERO server\n",
    "\n",
    "Excel files are converted into CSV files. <BR>\n",
    "SH files are generated to import images and their metadata.<BR>\n",
    "All files can be used to import images and their metadata into  SSBD-OMERO server (Ubuntu server).\n",
    "\n",
    "Input data: \n",
    "- Metadata_template.xlsx\n",
    "- project_name/project_name.xlsx\n",
    "\n",
    "Output data (e.g.):\n",
    "- metadata-all.csv (newly generated to check all generated metadata)<BR><BR>\n",
    "\n",
    "- metadata/copyData.sh (newly generated to upload all data in all projects into SSBD-OMERO server)\n",
    "- metadata/addAnnotations.sh (newly generated to import metadata of all images in all projects into SSBD-OMERO server)\n",
    "- metadata/makeMD5.sh (new generate to make MD5 files for all images into SSBD~OMERO server) <BR><BR>\n",
    "\n",
    "- metadata/project-name/project-name.xlsx (copy)\n",
    "- metadata/project-name/project-name.csv (newly generated to grasp all metadata in one project)\n",
    "- metadata/project-name/project-name_image_metadata.csv (newly generate to import metadata of images in one project into SSBD-OMERO server)\n",
    "- metadata/project-name/project-name_image.csv (newly generaeete to import images in one project into SSBD-OMERO server)\n",
    "- metadata/project-name/importImage.sh\n",
    "- metadata/project-name/zipImage.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "# > Python 3\n",
    "import pandas as pd\n",
    "import os, sys, csv, re, glob, shutil\n",
    "\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setting up the executing conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIR: Collection_test/\n",
      "OUTPUT_DIR: Collection_test/metadata/\n",
      "TEMPLATE_FILE: Collection_test/Metadata_template.xlsx\n",
      "TARGET_PROJECT: [20, 115]\n",
      "SERVER_HOST: hostname\n",
      "SERVER_USER: user\n",
      "SERVER_DIR: /home/user/metadata/\n"
     ]
    }
   ],
   "source": [
    "#INPUT_DIR = \"/Volumes/datacollection/Collection_test/\"\n",
    "INPUT_DIR = \"Collection_test/\"\n",
    "TEMPLATE_FILE = INPUT_DIR + 'Metadata_template.xlsx' # template file of metadata description\n",
    "\n",
    "#OUTPUT_DIR  = \"/Volumes/datacollection/Collection_test/metadata/\"\n",
    "OUTPUT_DIR  = \"Collection_test/metadata/\"\n",
    "\n",
    "# SSBD-OMERO server information\n",
    "SERVER_HOST  = \"hostname\" # hostname of ssbd-omero server\n",
    "SERVER_USER  = \"user\" # username of ssbd-omero server \n",
    "SERVER_DIR = \"/home/user/metadata/\" # direcotry of ssbd-omero server\n",
    "\n",
    "# Specify Project ID (PID)\n",
    "TARGET_PROJECTS = [20, 115]\n",
    "#TARGET_PROJECTS = [20]\n",
    "\n",
    "print (\"INPUT_DIR:\", INPUT_DIR)\n",
    "print (\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print (\"TEMPLATE_FILE:\", TEMPLATE_FILE)\n",
    "print (\"TARGET_PROJECT:\", TARGET_PROJECTS)\n",
    "print (\"SERVER_HOST:\", SERVER_HOST)\n",
    "print (\"SERVER_USER:\", SERVER_USER)\n",
    "print (\"SERVER_DIR:\", SERVER_DIR)\n",
    "\n",
    "if not os.path.isdir(INPUT_DIR):\n",
    "    print (\"ERROR: Input dir not accessed:\" + INPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding scale and unit on X,Y,Z-axes\n",
    "def searchXYZ (data):\n",
    "    if pd.isnull(data):\n",
    "        return \"0\", \"0\", \"0\", \"micrometer\"\n",
    "    \n",
    "    x_scale = \"-\"\n",
    "    y_scale = \"-\"\n",
    "    z_scale = \"-\"\n",
    "    x_unit  = \"-\"\n",
    "    y_unit  = \"-\"\n",
    "    z_unit  = \"-\"\n",
    "    \n",
    "    match = re.findall(r'([XYZ]+): ([\\d\\.]+) ([\\w\\.]+)', data)\n",
    "    for i in range(len(match)):\n",
    "        for s in list(match[i][0]):\n",
    "            if s == \"X\":\n",
    "                x_scale = match[i][1]\n",
    "                x_unit  = match[i][2]\n",
    "            elif s == \"Y\":\n",
    "                y_scale = match[i][1]\n",
    "                y_unit  = match[i][2]\n",
    "            elif s == \"Z\":\n",
    "                z_scale = match[i][1]\n",
    "                z_unit  = match[i][2]\n",
    "\n",
    "    match = re.findall(r'([XYZ]+): NA', data)\n",
    "    for i in range(len(match)):\n",
    "        for s in list(match[i][0]):\n",
    "            if s == \"X\":\n",
    "                x_scale = 0\n",
    "                if y_unit != \"-\":\n",
    "                    x_unit = y_unit\n",
    "                elif z_scale != \"-\":\n",
    "                    x_unit = z_unit\n",
    "            elif s == \"Y\":\n",
    "                y_scale = 0\n",
    "                if x_unit != \"-\":\n",
    "                    y_unit = x_unit\n",
    "                elif z_scale != \"-\":\n",
    "                    y_unit = z_unit\n",
    "            elif s == \"Z\":\n",
    "                z_scale = 0\n",
    "                if x_unit != \"-\":\n",
    "                    z_unit = x_unit\n",
    "                elif y_scale != \"-\":\n",
    "                    z_unit = y_unit\n",
    "\n",
    "    # ERROR: need to use the same unit among X,Y,Z-axes as precondition\n",
    "    if x_unit != y_unit or y_unit != z_unit:\n",
    "        x_unit = \"-\"\n",
    "\n",
    "    return x_scale, y_scale, z_scale, x_unit\n",
    "\n",
    "# Decoding scale and unit on T-axis\n",
    "def searchT (data):\n",
    "    if pd.isnull(data):\n",
    "        return \"0\", \"second\"\n",
    "\n",
    "    match = re.search(r\"([\\d\\.]+) ([\\w\\.]+)\", data)\n",
    "    if match:\n",
    "        if match.group(2) == \"days\":\n",
    "            return match.group(1), \"day\"\n",
    "        else:\n",
    "            return match.group(1), match.group(2)\n",
    "    else:\n",
    "        return \"-\", \"-\"\n",
    "\n",
    "# Decoding GO info.\n",
    "def searchOntology (data):\n",
    "    nlist = []\n",
    "    olist = []\n",
    "    \n",
    "    if pd.isnull(data):\n",
    "        return \"-\", \"-\"\n",
    "\n",
    "    for d in data.split(','):\n",
    "        match1 = re.search(r\"^([\\w\\d\\s\\.]+) \\(([\\w\\d]+):([\\w\\d]+)\\)\", d.strip())\n",
    "        match2 = re.search(r\"^([^\\(\\)]+)\", d.strip())\n",
    "        \n",
    "        if match1 and match1.group(1) != 'NA':\n",
    "            nlist.append(match1.group(1))\n",
    "            olist.append(match1.group(2) + \"_\" + match1.group(3))\n",
    "        elif match2 and match2.group(1) != 'NA':\n",
    "            nlist.append(match2.group(1))\n",
    "            olist.append(\"-\")\n",
    "            \n",
    "    if len(nlist) >= 1:\n",
    "        return ', '.join(nlist), ', '.join(olist)\n",
    "    else:\n",
    "        return \"-\", \"-\"\n",
    "\n",
    "# Decoding PubMed info.\n",
    "def searchPubMedID (data):\n",
    "    if pd.isnull(data):\n",
    "        return \"-\"    \n",
    "    match = re.search(r\"^https://www.ncbi.nlm.nih.gov/pubmed/(\\d+)$\", data)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"-\"\n",
    "\n",
    "# Reading all items of \"common\" sheet in Metadata_template.xlsx\n",
    "def readTemplate (template_file):\n",
    "    all_labels = []\n",
    "    if not os.path.isfile(template_file):\n",
    "        print (\"ERROR: Template file not found. \" + template_file)\n",
    "    try:\n",
    "        template = pd.read_excel(template_file, sheet_name='common')\n",
    "        all_labels = list(template['Item'])\n",
    "    except:\n",
    "        print (\"Cannot access:\", template_file)\n",
    "        \n",
    "    return all_labels\n",
    "\n",
    "# Checking consistency of all items of \"common\" sheet between Metadata_template.xlsx and Metadata.xlsx for one project\n",
    "def checkFormat (input_file, template_file):    \n",
    "    all_labels = readTemplate(template_file)\n",
    "    if not all_labels:\n",
    "        print (\"Error of template excel\")\n",
    "        return -1\n",
    "    \n",
    "    try:\n",
    "        data1 = pd.read_excel(input_file, sheet_name='common')\n",
    "    except:\n",
    "        print (\"Cannot access common sheet:\", input_file)\n",
    "        return -1\n",
    "    \n",
    "    # Checking all items\n",
    "    if list(data1['Item']) != all_labels:\n",
    "        for label in list(data1['Item']):\n",
    "            if not label in all_labels:\n",
    "                print (\"Error in common sheet\", label)\n",
    "                return -1\n",
    "                    \n",
    "    # Checking PubMed URL\n",
    "    url = data1[data1['Item'].isin(['PubMed URL'])].iloc[0,1]\n",
    "    if url != 'NA' and not pd.isnull(url):\n",
    "        match = re.findall(r\"https://www.ncbi.nlm.nih.gov/pubmed/(\\d+)$\", url)\n",
    "        if len(match) != 1:\n",
    "            print (\"Error of PubMed URL in common sheet\")\n",
    "            return -1\n",
    "    \n",
    "    # Selecting items, in which all datasets have nothing in common\n",
    "    tmp = data1[data1['Description'].isin(['Different by every dataset (refer to dataset sheet)'])]\n",
    "    diff_count = len(tmp.index)\n",
    "    diff_labels = []\n",
    "    if diff_count != 0:\n",
    "        diff_labels = list(tmp['Item'])\n",
    "        \n",
    "    # Cheching existence of the selected items in \"dataset\" sheet, excepting for \"Note\" item\n",
    "    try:\n",
    "        data2 = pd.read_excel(input_file, sheet_name='dataset')\n",
    "        for label in diff_labels:\n",
    "            if not label in list(data2.columns):\n",
    "                print (\"Error of %s in dataset sheet\" % label)\n",
    "                return -1\n",
    "    except:\n",
    "        print (\"Cannot access dataset sheet:\", input_file)\n",
    "        return -1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Collecting and sorting metadata info for each dataset \n",
    "def decodeDataset(data2, dataset, dic1, all_labels):\n",
    "    dic2 = {}\n",
    "    dic2['dataset-name']  = \"-\"\n",
    "    dic2['xScale']  = \"-\"\n",
    "    dic2['yScale']  = \"-\"\n",
    "    dic2['zScale']  = \"-\"\n",
    "    dic2['xyzUnit'] = \"-\"\n",
    "    dic2['tScale']  = \"-\"\n",
    "    dic2['tUnit']   = \"-\"\n",
    "    dic2['PMID']    = \"-\"\n",
    "    dic2['organism-name']          = \"-\"\n",
    "    dic2['organism_ontology'] = \"-\"\n",
    "    dic2['biological-process']          = \"-\"\n",
    "    dic2['biological-process_ontology'] = \"-\"\n",
    "    dic2['cellular-component']          = \"-\"\n",
    "    dic2['cellular-component_ontology'] = \"-\"\n",
    "    \n",
    "    # Copying \"common\" sheet\n",
    "    for label in all_labels:\n",
    "        dic2[label] = \"-\"\n",
    "        if dic1[label] != None and dic1[label] != \"-\":\n",
    "            dic2[label] = dic1[label]\n",
    "        elif label in data2.columns and not pd.isnull(dataset[label]) and dataset[label] != \"NA\":\n",
    "            dic2[label] = dataset[label]\n",
    "    \n",
    "    # dataset-name の処理\n",
    "    dic2['dataset-name'] = dic2['LocalID']\n",
    "    \n",
    "    # Pattern matching\n",
    "    source_default    = 'http://ssbd.qbic.riken.jp/data/(Project name)/source/(LocalID).zip'\n",
    "    bdml_default      = 'http://ssbd.qbic.riken.jp/data/(Project name)/bdml/(LocalID)_bdml3.0.zip'\n",
    "    omicsbdml_default = 'http://ssbd.qbic.riken.jp/data/(Project name)/bdml/(LocalID)_omicsbdml1.0.zip'\n",
    "    pdpml_default     = 'http://ssbd.qbic.riken.jp/data/pdpml/(Project name)_pdpml1.0.xml'\n",
    "    for label in all_labels:\n",
    "        if label == 'Description' and dic2[label] == \"-\":\n",
    "            if dic2['Kind'] == \"Image data\":\n",
    "                dic2['Description'] = dic2['Title']\n",
    "            elif dic2['Kind'] == \"Quantitative data\":\n",
    "                match = re.search(r\"BDML file for (.+)$\", dic2['Title'])\n",
    "                if match:\n",
    "                    dic2['Description'] = match.group(1)\n",
    "                else:\n",
    "                    dic2['Description'] = dic2['Title']\n",
    "                \n",
    "        elif label == 'Source' and dic2[label] == source_default:\n",
    "            if dic2['Kind'] == \"Image data\" :\n",
    "                dic2[label] = 'http://ssbd.qbic.riken.jp/data/%s/source/%s.zip' % (dic2['Project name'], dic2['LocalID'])\n",
    "                \n",
    "            # For quantitative data, searching the corresponding to a set of images\n",
    "            elif dic2['Kind'] == \"Quantitative data\":\n",
    "                if len(data2.index) > 1:\n",
    "                    b = []\n",
    "                    b = data2[(data2['Kind'] == \"Image data\") & (data2['LocalID'] == dic2['LocalID'])].index # 組になる画像を探す\n",
    "                    if len(b) == 1: # Exist\n",
    "                        dic2[label] = 'http://ssbd.qbic.riken.jp/data/%s/source/%s.zip' % (dic2['Project name'], dic2['LocalID'])    \n",
    "                    elif len(b) == 0: # Not exist\n",
    "                        dic2[label] = \"-\"\n",
    "                    else:\n",
    "                        dic2[label] = \"error1\"\n",
    "                else:\n",
    "                    dic2[label] = \"error2\"\n",
    "\n",
    "        elif label == 'BDML' and (dic2[label] == bdml_default or dic2[label] == omicsbdml_default):\n",
    "            if dic2['Kind'] == \"Quantitative data\":\n",
    "                if dic2[label] == bdml_default:\n",
    "                    dic2[label] = 'http://ssbd.qbic.riken.jp/data/%s/bdml/%s_bdml3.0.zip' % (dic2['Project name'], dic2['LocalID'])\n",
    "                elif dic2[label] == omicsbdml_default:\n",
    "                    dic2[label] = 'http://ssbd.qbic.riken.jp/data/%s/bdml/%s_omicsbdml1.0.zip' % (dic2['Project name'], dic2['LocalID'])\n",
    "\n",
    "            # For image data, searching the corresponding to a quantitative data\n",
    "            elif dic2['Kind'] == \"Image data\":\n",
    "                if len(data2.index) > 1:\n",
    "                    b = []\n",
    "                    b = data2[(data2['Kind'] == \"Quantitative data\") & (data2['LocalID'] == dic2['LocalID'])].index\n",
    "                    if len(b) == 1: # Exist\n",
    "                        if dic2[label] == bdml_default:\n",
    "                            dic2[label] = 'http://ssbd.qbic.riken.jp/data/%s/bdml/%s_bdml3.0.zip' % (dic2['Project name'], dic2['LocalID'])\n",
    "                        elif dic2[label] == omicsbdml_default:\n",
    "                            dic2[label] = 'http://ssbd.qbic.riken.jp/data/%s/bdml/%s_omicsbdml1.0.zip' % (dic2['Project name'], dic2['LocalID'])\n",
    "                    elif len(b) == 0: # Not exist\n",
    "                        dic2[label] = \"-\"\n",
    "                    else:\n",
    "                        dic2[label] = \"error1\"\n",
    "                else:\n",
    "                    dic2[label] = \"error2\"\n",
    "                    \n",
    "        elif label == 'PDPML' and dic2[label] == pdpml_default:\n",
    "            dic2[label] = 'http://ssbd.qbic.riken.jp/data/pdpml/%s_pdpml1.0.xml' % (dic2['Project name'])  \n",
    "        elif label == 'XYZ-scale':\n",
    "            dic2['xScale'], dic2['yScale'], dic2['zScale'], dic2['xyzUnit'] = searchXYZ(dic2[label])\n",
    "        elif label == 'T-scale':\n",
    "            dic2['tScale'], dic2['tUnit'] = searchT(dic2[label])\n",
    "        elif label == 'PubMed URL':\n",
    "            dic2['PMID'] = searchPubMedID(dic2[label])\n",
    "        elif label == 'Organism':\n",
    "            d1, d2 = searchOntology(dic2[label])\n",
    "            dic2['organism'] = d1\n",
    "            dic2['organism_ontology'] = d2\n",
    "        elif label == 'Biological Process':\n",
    "            d1, d2 = searchOntology(dic2[label])\n",
    "            if d1 != '-' and d2 != '-':\n",
    "                dic2['biological-process'] = d1\n",
    "                dic2['biological-process_ontology'] = d2\n",
    "        elif label == 'Cellular Component':\n",
    "            d1, d2 = searchOntology(dic2[label])\n",
    "            if d1 != '-' and d2 != '-':\n",
    "                dic2['cellular-component'] = d1\n",
    "                dic2['cellular-component_ontology'] = d2\n",
    "\n",
    "    return dic2\n",
    "\n",
    "def decodeMetadata(input_file, template_file, output_labels, target_kind, writer1, writer2):\n",
    "    all_labels = readTemplate(template_file)\n",
    "    \n",
    "    if not os.path.isfile(input_file):\n",
    "        print (\"ERROR: Input file not found. \" + input_file)\n",
    "        return -1\n",
    "\n",
    "    try:\n",
    "        data1 = pd.read_excel(input_file, sheet_name='common')\n",
    "    except:\n",
    "        print (\"Cannot access common sheet:\", input_file)\n",
    "        return -1\n",
    "    \n",
    "    # Creating dictionary on common sheet\n",
    "    dic1 = {}\n",
    "    diff_count = 0\n",
    "    for label in all_labels:\n",
    "        dic1[label]  = data1[data1['Item'].isin([label])].iloc[0,1]\n",
    "        if dic1[label]  == 'Different by every dataset (refer to dataset sheet)':\n",
    "            dic1[label] = None\n",
    "            diff_count += 1\n",
    "        elif pd.isnull(dic1[label]):\n",
    "            dic1[label]  = \"-\"\n",
    "             \n",
    "    # Creating dataset info. if there is no derscription of \"Different ...\" in \"common\" sheet\n",
    "    if diff_count == 0:\n",
    "        data2 = pd.DataFrame({'Kind': [dic1['Kind']], 'LocalID': [dic1['LocalID']] })\n",
    "    # ある場合は dataset シートを読む\n",
    "    else:\n",
    "        try:\n",
    "            data2 = pd.read_excel(input_file, sheet_name='dataset')        \n",
    "        except:\n",
    "            print (\"Cannot access dataset sheet:\", input_file)\n",
    "            return -1\n",
    "\n",
    "    # Decoding dataset info. in \"dataset\" sheet row by row\n",
    "    dataset_list = []\n",
    "    for index, dataset in data2.iterrows():\n",
    "        if dic1['Kind'] == target_kind or (dic1['Kind'] == None and dataset['Kind'] == target_kind):\n",
    "            dic2 = decodeDataset(data2, dataset, dic1, all_labels)\n",
    "            #print (dic2['dataset-name'])\n",
    "            \n",
    "            csv_list = []\n",
    "            for label in output_labels:\n",
    "                dic2[label] = dic2[label]\n",
    "                csv_list.append(dic2[label])\n",
    "\n",
    "            if len(csv_list) >= 1:\n",
    "                writer1.writerow(csv_list)\n",
    "                writer2.writerow(csv_list)\n",
    "                dataset_list.append(dic2['dataset-name'])\n",
    "\n",
    "    return len(dataset_list), dataset_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating CSV and SH files to import images and their metadata into SSBD-OMERO server\n",
    "def readImageDataCSV(input_dir, output_dir, ssbd_dir, project_name):\n",
    "    input_p_dir    = input_dir + project_name + '/'\n",
    "    input_p_file   = output_dir + project_name + '/' + project_name + '.csv'\n",
    "\n",
    "    output_p_dir   = output_dir + project_name + '/'\n",
    "    output_p_file1 = output_p_dir + project_name + '_image_metadata.csv' # CSV files to import metadata of images\n",
    "    output_p_file2 = output_p_dir + project_name + '_image.csv' # CSV files to import images\n",
    "    output_p_file3 = output_p_dir + 'importImage.sh' # Script to import images\n",
    "    output_p_file4 = output_p_dir + 'zipImage.sh' # Script to zip images\n",
    "\n",
    "    ssbd_p_dir     = ssbd_dir + project_name + '/source/'\n",
    "\n",
    "    if not os.path.isfile(input_p_file):\n",
    "        print (\"ERROR: Input CSV file not found:\" + input_p_file)\n",
    "        return project_name, 0, 0, {}\n",
    "    \n",
    "    df1 = pd.read_csv(input_p_file)\n",
    "    \n",
    "    df2 = df1[df1['kind'] == \"Image data\"]\n",
    "\n",
    "    if not df2.empty and df2.shape[0] >= 1:\n",
    "        # Checking for the existence of \"source\" directory\n",
    "        if not os.path.isdir(input_p_dir):\n",
    "            print (\"ERROR: Input dir not found:\" + input_p_dir)\n",
    "            return project_name, 0, 0, {}\n",
    "        \n",
    "    # Creating files.\n",
    "    fp1 = csv.writer(open(output_p_file1, \"w\"), lineterminator='\\n')\n",
    "    fp2 = csv.writer(open(output_p_file2, \"w\"), lineterminator='\\n')\n",
    "    fp3 = open(output_p_file4, \"w\")\n",
    "    fp3.writelines(\"#!/bin/sh\\n\")\n",
    "\n",
    "    org_dic = {} #the number of the datasets for each kind of the organism\n",
    "    total_image_count = 0 # the number of image files for each project\n",
    "\n",
    "    for i in df2.index:\n",
    "        dataset_name = df2.iloc[i][\"localID\"]\n",
    "        input_d_dir = input_p_dir + 'source/' + dataset_name + '/'\n",
    "        if not os.path.isdir(input_d_dir):\n",
    "            print (\"Not exist\", input_d_dir)\n",
    "        else:\n",
    "            image_count = 0\n",
    "            ext_dic   = {}\n",
    "            for file_name in os.listdir(input_d_dir):\n",
    "                if os.path.isfile(input_d_dir + file_name) and file_name != '.DS_Store':\n",
    "                    ext = os.path.splitext( file_name )[1]\n",
    "                    if ext in ext_dic.keys():\n",
    "                        ext_dic[ext] += 1\n",
    "                    else:\n",
    "                        ext_dic[ext] = 1\n",
    "                    image_count += 1\n",
    "                    \n",
    "            if image_count > 0:\n",
    "                total_image_count += image_count\n",
    "            \n",
    "            # Generating CSV file (TODO: commonalize CSV files)\n",
    "            dic = df2.iloc[i].to_dict()\n",
    "            csvlist1 = [dic['project-name'], dic['dataset-name'], dic['dataset-name'], dic['summary'], dic['license'], dic['contact-name'], \\\n",
    "                        dic['organization'], dic['department'], dic['laboratory'], dic['contributors'], dic['title'], dic['organism'], \\\n",
    "                        dic['PMID'] , dic['x-scale'], dic['y-scale'], dic['z-scale'], dic['xyz-unit'], dic['t-scale'], dic['t-unit'], \\\n",
    "                        dic['source'], dic['bdml'], dic['exlinks']]\n",
    "            fp1.writerow(csvlist1)\n",
    "            #print (csvlist1)\n",
    "\n",
    "            # Summarzing the number of organism\n",
    "            if dic['organism'] in org_dic:\n",
    "                org_dic[dic['organism']] += 1\n",
    "            else:\n",
    "                org_dic[dic['organism']] = 1\n",
    "\n",
    "            # Generating import commands (project_name, dataset_name, file extension, #images)\n",
    "            ssbd_d_dir = ssbd_p_dir + dataset_name + \"/\"\n",
    "            if len(ext_dic.keys()) != 1:\n",
    "                print (\"Extension ERROR in Project\", project_name, \"Dataset\", dataset_name, \":\", ext_dic.keys())\n",
    "            csvlist2 = [dic['project-name'], dic['dataset-name'], ext, ssbd_d_dir, str(image_count)]\n",
    "            fp2.writerow(csvlist2)\n",
    "        \n",
    "            # Generating zip commands\n",
    "            cmd = \"cd source; rm %s.zip; zip %s.zip -r %s\\n\" % (dataset_name, dataset_name, dataset_name)\n",
    "            fp3.writelines(cmd)\n",
    "\n",
    "    fp3.close()\n",
    "    \n",
    "    # Generating script to import images for each project\n",
    "    if len(df2.index) > 0:\n",
    "        fp4 = open(output_p_file3, 'w')\n",
    "        fp4.writelines(\"#!/bin/sh\\n\")\n",
    "        cmd = 'python /OMERO/importImages.py %s_image.csv' % (project_name)\n",
    "        fp4.writelines(cmd)\n",
    "        fp4.close()\n",
    "        \n",
    "    else:\n",
    "        if os.path.isfile(output_p_file1):\n",
    "            os.remove(output_p_file1)\n",
    "        if os.path.isfile(output_p_file2):\n",
    "            os.remove(output_p_file2)\n",
    "        if os.path.isfile(output_p_file3):\n",
    "            os.remove(output_p_file3)\n",
    "        if os.path.isfile(output_p_file4):\n",
    "            os.remove(output_p_file4)\n",
    "        \n",
    "    return len(df2.index), total_image_count, org_dic\n",
    "\n",
    "def getProjects (input_dir, project_no):\n",
    "    pattern = re.compile(r'\\d+')\n",
    "    datasets = []\n",
    "    if len(project_no) != 0:\n",
    "        path_dict = {}\n",
    "        file_dict = {}\n",
    "        for project_name in sorted(os.listdir(input_dir)):\n",
    "            full_path = input_dir + project_name + \"/\"\n",
    "            if os.path.isdir(full_path): \n",
    "                match_obj = pattern.match(project_name) \n",
    "                if match_obj and int(match_obj.group(0)) in project_no:\n",
    "                    datasets.append(project_name)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check for existance of the EXCEL file described metadata per project\n",
    "\n",
    "Precondition: Project name is equal to EXCEL-file name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIR: Collection_test/\n",
      "OK: 20-Azuma-WormMembrane\n",
      "Directory name error: metadata\n",
      "Target: [20]\n",
      "Not target: []\n"
     ]
    }
   ],
   "source": [
    "print (\"INPUT_DIR:\", INPUT_DIR)\n",
    "\n",
    "ok_projects = []\n",
    "tmp_projects = []\n",
    "\n",
    "pattern = re.compile(r'\\d+')\n",
    "\n",
    "dir_dict = {}\n",
    "file_dict = {}\n",
    "for project_name in sorted(os.listdir(INPUT_DIR)):\n",
    "    input_p_dir = INPUT_DIR + project_name + \"/\"\n",
    "    input_p_file = INPUT_DIR + project_name + \"/\" + project_name + \".xlsx\"\n",
    "    \n",
    "    if os.path.isdir(input_p_dir):\n",
    "        match_obj = pattern.match(project_name)\n",
    "        if match_obj:\n",
    "            if int(match_obj.group(0)) in TARGET_PROJECTS:         \n",
    "                if os.path.isfile(input_p_file): \n",
    "                    print (\"OK:\", project_name)\n",
    "                    ok_projects.append(int(match_obj.group(0)))\n",
    "                    dir_dict[project_name] = input_p_dir\n",
    "                    file_dict[project_name] = input_p_file\n",
    "                else: \n",
    "                    print (\"Excel-file name error:\", os.path.basename(input_p_file))\n",
    "            else:\n",
    "                tmp_projects.append(int(match_obj.group(0)))\n",
    "        else:\n",
    "            print (\"Directory name error:\", project_name)\n",
    "\n",
    "print (\"Target:\", sorted(ok_projects))\n",
    "print (\"Not target:\", sorted(tmp_projects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check format of the EXCEL file based on Metadata_template.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPLATE_FILE: Collection_test/Metadata_template.xlsx\n",
      "Available terms in CSV:\n",
      "\t project-name (xlsx: Project name )\n",
      "\t dataset-name\n",
      "\t localID (xlsx: LocalID )\n",
      "\t contact-name (xlsx: Contact name )\n",
      "\t E-mail (xlsx: E-mail )\n",
      "\t organization (xlsx: Organization )\n",
      "\t department (xlsx: Department )\n",
      "\t laboratory (xlsx: Laboratory )\n",
      "\t address (xlsx: Address )\n",
      "\t license (xlsx: License )\n",
      "\t contributors (xlsx: Contributors )\n",
      "\t summary (xlsx: Method summary )\n",
      "\t PMID\n",
      "\t kind (xlsx: Kind )\n",
      "\t title (xlsx: Title )\n",
      "\t description (xlsx: Description )\n",
      "\t organism\n",
      "\t organism_ontology\n",
      "\t datatype (xlsx: Datatype )\n",
      "\t basedon (xlsx: Basedon )\n",
      "\t x-scale\n",
      "\t y-scale\n",
      "\t z-scale\n",
      "\t xyz-unit\n",
      "\t t-scale\n",
      "\t t-unit\n",
      "\t dblink (xlsx: dblink )\n",
      "\t exlinks (xlsx: External links )\n",
      "\t source (xlsx: Source )\n",
      "\t bdml (xlsx: BDML )\n",
      "\t pdpml (xlsx: PDPML )\n",
      "\t biological-process\n",
      "\t biological-process_ontology\n",
      "\t cellular-component\n",
      "\t cellular-component_ontology\n",
      "OK Collection_test/20-Azuma-WormMembrane/20-Azuma-WormMembrane.xlsx\n",
      "#project = 1\n",
      "#error = 0\n"
     ]
    }
   ],
   "source": [
    "# Mappping item names of EXCEL file to CSV file\n",
    "# Item name for EXCEL\n",
    "output_labels1 = ['Project name', 'dataset-name', 'LocalID', 'Contact name', 'E-mail', 'Organization', 'Department', \\\n",
    "                 'Laboratory', 'Address', 'License', 'Contributors', 'Method summary', 'PMID', \\\n",
    "                 'Kind', 'Title', 'Description', 'organism', 'organism_ontology', 'Datatype', 'Basedon', \\\n",
    "                 'xScale', 'yScale', 'zScale', 'xyzUnit', 'tScale', 'tUnit', 'dblink', 'External links', \\\n",
    "                 'Source', 'BDML', 'PDPML', \\\n",
    "                 'biological-process', 'biological-process_ontology', 'cellular-component', 'cellular-component_ontology']\n",
    "\n",
    "# Item name for CSV\n",
    "output_labels2 = ['project-name', 'dataset-name', 'localID', 'contact-name', 'E-mail', 'organization', 'department', \\\n",
    "                 'laboratory', 'address', 'license', 'contributors', 'summary', 'PMID', \\\n",
    "                 'kind', 'title', 'description', 'organism', 'organism_ontology', 'datatype', 'basedon', \\\n",
    "                 'x-scale', 'y-scale', 'z-scale', 'xyz-unit', 't-scale', 't-unit', 'dblink', 'exlinks', \\\n",
    "                 'source', 'bdml', 'pdpml', \\\n",
    "                 'biological-process', 'biological-process_ontology', 'cellular-component', 'cellular-component_ontology']\n",
    "\n",
    "print (\"TEMPLATE_FILE:\", TEMPLATE_FILE)\n",
    "term_list = readTemplate(TEMPLATE_FILE)\n",
    "\n",
    "if len(output_labels1) != len(output_labels2):\n",
    "    print (\"Error: cannot correspond output_labels1 and output_label2\")\n",
    "\n",
    "print (\"Available terms in CSV:\")\n",
    "for i in range(len(output_labels1)):\n",
    "    if not output_labels1[i] in term_list:\n",
    "        print (\"\\t\", output_labels2[i])\n",
    "    else:\n",
    "        print (\"\\t\", output_labels2[i], \"(xlsx:\", output_labels1[i], \")\")\n",
    "\n",
    "error = 0\n",
    "count = 0\n",
    "for project_name, input_p_file in file_dict.items():\n",
    "    result = checkFormat(input_p_file, TEMPLATE_FILE) # Metadata_template と同じ形式かをチェック\n",
    "    if result == 0:\n",
    "        print (\"OK\", input_p_file)\n",
    "    else:\n",
    "        print (\"NG\", input_p_file)\n",
    "        error += result\n",
    "    count += 1       \n",
    "\n",
    "print (\"#project = %d\" % count)\n",
    "print (\"#error = %d\" % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generating all CSV and SH files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory was created: Collection_test/metadata/\n",
      "Targets: [20, 115]\n",
      "20-Azuma-WormMembrane\n",
      "\tImage data: #1\n",
      "\tQuantitative data: #1\n",
      "\tneeded scp: Collection_test/20-Azuma-WormMembrane/source/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True) #  # Creating working directory if not exist\n",
    "\n",
    "print (\"Output directory was created:\", OUTPUT_DIR)\n",
    "print (\"Targets:\", TARGET_PROJECTS)\n",
    "    \n",
    "output_file1 = OUTPUT_DIR + 'metadata-all.csv'\n",
    "output_file2 = OUTPUT_DIR + 'copyData.sh'\n",
    "output_file5 = OUTPUT_DIR + 'addAnnotations.sh'\n",
    "output_file6 = OUTPUT_DIR + 'makeMD5.sh'\n",
    "\n",
    "fp1 = open(output_file1, \"w\") # Creating CSV file [metadata-all.csv] including metadata for all projects \n",
    "writer1 = csv.writer(fp1, lineterminator='\\n')\n",
    "writer1.writerow(output_labels2)\n",
    "\n",
    "fp2 = open(output_file2, \"w\") # Creating SH script [copyData.sh] to copy metadata into SSBD-OMERO server\n",
    "fp2.write('#!/bin/sh\\n')\n",
    "fp2.write('scp -r %s* %s@%s:%s.\\n' % (OUTPUT_DIR, SERVER_USER, SERVER_HOST, SERVER_DIR))\n",
    "\n",
    "# Processing for each project\n",
    "for project_name, full_file in file_dict.items():\n",
    "    input_file   = INPUT_DIR + project_name + '/' + project_name + '.xlsx'\n",
    "    input_p_dir  = INPUT_DIR + project_name + '/'\n",
    "        \n",
    "    output_p_dir = OUTPUT_DIR + project_name + '/'\n",
    "    output_file3 = output_p_dir + project_name + '.xlsx'\n",
    "    output_file4 = output_p_dir + project_name + '.csv' \n",
    "        \n",
    "    os.makedirs(output_p_dir, exist_ok=True) # Creating project directory if not exist\n",
    "        \n",
    "    print(project_name)\n",
    "        \n",
    "    # Creating CSV file including metadata for one target project\n",
    "    fp4 = open(output_file4, \"w\")\n",
    "    writer2 = csv.writer(fp4, lineterminator='\\n') #, quoting=csv.QUOTE_NONE\n",
    "    writer2.writerow(output_labels2) # 項目名の出力\n",
    "                \n",
    "    # Outputing information of images data and quantitative data per project\n",
    "    for kind in [\"Image data\", \"Quantitative data\"]:\n",
    "        (n, dataset_list) = decodeMetadata(full_file, TEMPLATE_FILE, output_labels1, kind, writer1, writer2)\n",
    "        print (\"\\t%s: #%d\" % (kind, n))\n",
    "        if n > 0:\n",
    "            for dataset_name in dataset_list:\n",
    "                input_d_dir = \"\"\n",
    "                if kind == \"Image data\":\n",
    "                    input_d_dir = input_p_dir + 'source/' + dataset_name + '/'\n",
    "                elif kind == \"Quantitative data\":\n",
    "                    input_d_dir = input_p_dir + 'bdml/' + dataset_name + '/'\n",
    "\n",
    "                if not os.path.isdir(input_d_dir):\n",
    "                    print (\"\\t\\tNot exist\", input_d_dir)\n",
    "        \n",
    "    fp4.close()\n",
    "\n",
    "    # Adding scp command into script [copyData.sh] to copy source folders into SSBD-OMERO server\n",
    "    dir2 = '%s@%s:%s%s/.' % (SERVER_USER, SERVER_HOST, SERVER_DIR, project_name)\n",
    "    dir1 = input_p_dir + 'data/'\n",
    "    if os.path.isdir(dir1):\n",
    "        print (\"\\tneeded scp:\", dir1)\n",
    "        fp2.write(\"scp -r %s %s\\n\" % (dir1, dir2))\n",
    "    dir1 = input_p_dir + 'source/'\n",
    "    if os.path.isdir(dir1):\n",
    "        print (\"\\tneeded scp:\", dir1)\n",
    "        fp2.write(\"scp -r %s %s\\n\" % (dir1, dir2))\n",
    "    \n",
    "    shutil.copy(input_file, output_file3) # Copying EXCEL files\n",
    "    \n",
    "fp1.close()\n",
    "fp2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-Azuma-WormMembrane , #dataset: 1 , #image: 1 {'C. elegans': 1}\n",
      "\n",
      "The following CSV files and scripts were created:\n",
      "Collection_test/metadata/metadata-all.csv\n",
      "Collection_test/metadata/copyData.sh\n",
      "Collection_test/metadata/addAnnotations.sh\n",
      "Collection_test/metadata/makeMD5.sh\n",
      "\n",
      "Total #project: 1 , Total #dataset of images: 1 , Total #image: 1\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "fp5 = open(output_file5, 'w')\n",
    "fp6 = open(output_file6, 'w')\n",
    "\n",
    "fp5.writelines('#!/bin/sh\\n\\n')\n",
    "fp6.writelines('#!/bin/zsh\\n\\ndirs=(\\\\\\n')\n",
    "\n",
    "total_imageset_count = 0\n",
    "total_image_count = 0\n",
    "project_list = getProjects(OUTPUT_DIR, TARGET_PROJECTS)\n",
    "for project_name in project_list:\n",
    "    imageset_count, image_count, org_dic  = readImageDataCSV(INPUT_DIR, OUTPUT_DIR, SERVER_DIR, project_name)\n",
    "    print (project_name, \", #dataset:\", imageset_count, \", #image:\", image_count, org_dic)\n",
    "    \n",
    "    # Generating script to annotate metadata of images on OMERO\n",
    "    if imageset_count != 0:\n",
    "        cmd1 = \"python /OMERO/addAnnotations.py \" + SERVER_DIR + project_name + \"/\" + project_name + \"_image_metadata.csv\" + \"\\n\"\n",
    "        cmd2 = \"\\t\\\"\" + SERVER_DIR + project_name + \"/source/\\\"\\\\\\n\"\n",
    "        fp5.writelines(cmd1)\n",
    "        fp6.writelines(cmd2)\n",
    "\n",
    "    total_imageset_count += imageset_count\n",
    "    total_image_count += image_count\n",
    "\n",
    "    fp6.writelines('\\t)\\n\\nfor dir in ${dirs}; do\\n\\techo ${dir}\\n\\tcd ${dir}\\n\\tfor file in `find . -name \"*.zip\" | sort`;\\n\\tdo\\n\\t\\tmd5_file=\"${file}.md5\"\\n\\t\\t/usr/bin/md5sum $file > $md5_file\\n\\t\\techo $md5_file\\n\\tdone;\\ndone;\\n')\n",
    "\n",
    "fp5.close()\n",
    "fp6.close()\n",
    "\n",
    "print (\"\\nThe following CSV files and scripts were created:\")\n",
    "print (output_file1)\n",
    "print (output_file2)\n",
    "print (output_file5)\n",
    "print (output_file6)\n",
    "\n",
    "print (\"\\nTotal #project:\", len(project_list), \", Total #dataset of images:\", total_imageset_count, \", Total #image:\", total_image_count)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
